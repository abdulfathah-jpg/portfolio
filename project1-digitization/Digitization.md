---
title: Digitization
permalink: /digitization/
layout: page
image: images/Digitization.jpg
---

<!--more-->

The primary goals of the mini project 1 has been to digitize an important 20th century publication from the Shia Imami Ismaili community in Gilgit-Baltistan. The manuscript, available as a hard copy in the Ismaili Special Collections unit of the Aga Khan Library (London) has been processed digitally to ensure its longtime preservation and broader accessibility to researchers, community members, and digital archives. On a technical level, a secondary goal has been to ensure that students master the entire digitization workflow and that they have been introduced into essential digital humanities skills such as Python basics, GitHub, OCR process, and Metadata curation based on archival standards and Library of Congress transliteration guidelines. Besides the technical exercise, the project contributes to a shared digital repository, namely the Aga Khan Library digital collections, and thus reinforce ideas of heritage preservation, community archiving, and shared scholarly responsibility to make local archives part of global digital humanities landscape. 

![Manuscript Cover Page]({{site.baseurl}}images/Rasail002.jpg)

The digitized work, Kitāb-i Tuḥfatah al-Nāẓirīn al-Maʿrūf Ṣaḥīfah bi-Inẓimām Sharḥ al-Marātib va Davāzdah Faṣl az Taḍyīfāt Sayyid Suhrāb Raḍavī al-Badakhshānī, is a rare twentieth-century Ismaili publication produced in Gilgit and printed by Intishārāt-i Ismāʿīliyyah, Idārat al-Balāghah, Markaz-i Gilgit around 1940. The 164-page manuscript, basically a commentary and compilation of some earlier work, has been arranged by Qudrat Allāh Beg and edited by Ghulām al-Dīn. The work is significant for its engagement with themes of esoteric theology, gnostic interpretation, and cosmological hierarchy in Ismailism, as well as for understanding the evolution of religious literature in the mountain societies of northern Pakistan. The hard copy of the manuscript reached us in a very fragile form without distinctive covers, thus making the digitization of this important artifact of Ismaili intellectual heritage an immediate imperative. 

The manuscript was scanned using an overhead document scanner provided by the Aga Khan Library within the classroom setting. The scanning process was carried out under the supervision of a digitization expert and completed collaboratively by group members. Foam wedges and book snakes were used to support the manuscript and ensure minimal strain on its binding while achieving flat, stable images of each page. Because the scanner employed was a modest, single-surface model, each page had to be manually positioned and captured individually, rather than scanned in double-page format from the gutter. The collected images were subsequently put into a cleaning procedure that checked for any blur, duplication or partial scans. Defective scans, which amounted to a total of eleven, were replaced with new images, extraneous desk backgrounds were cropped, and all files were renamed sequentially and standardized in format (.jpg) using a Python-based renaming script.

For the metadata file created by the group, see [Metadata File](https://raw.githubusercontent.com/abdulfathah-jpg/portfolio/refs/heads/master/project1-digitization/Rasail_Metadata.txt)

For the Table of Contents of the Manuscript in CSV formate, see [Table of Contents](https://raw.githubusercontent.com/https://raw.githubusercontent.com/abdulfathah-jpg/portfolio/refs/heads/master/project1-digitization/Table_of_contents.csv)


The next crucial stage involved uploading the cleaned image set to eScriptorium, an open-source platform designed for the transcription, annotation, and training of Handwritten Text Recognition (HTR) models. The images were first segmented using a fine-tuned model previously trained on a sample set of pages whose text regions had been manually annotated and labeled. Region labeling followed a region annotation guideline curated specifically for this project, drawing on the SegmOnto standard for Zones to ensure consistency and interoperability. 

[Segmentation Guidelines](https://github.com/abdulfathah-jpg/portfolio/blob/master/project1-digitization/segmentation_guidelines_DH25.pdf)

Following a modestly successful segmentation process, the manuscript’s Persian text was transcribed using the Kraken model gen2-print-n7m5-union-ft_best, which achieved the highest average Character Accuracy Rate (CAR) of 0.83 among four models tested. This evaluation was performed through a Python script employing the error_rate function to compute Character Error Rate (CER) and Word Error Rate (WER) against the ground-truth transcription, with mean values used to compare model performance. For digitizing Persian manuscripts, character-level metrics (CAR/CER) offer a more reliable performance indicator than word-level metrics (WER), given the frequent orthographic variations, ligatures, and damaged glyphs characteristic of such texts. The final transcribed outputs were then exported as both image overlays and XML files ready for further curation and digital publication.

For the complete Python script used to evaluate the mean CER/CAR, see [Python script for Evaluation](https://raw.githubusercontent.com/abdulfathah-jpg/portfolio/refs/heads/master/project1-digitization/CER-CAR-Evaluation/fathah_evaluate_transcription.py)

For a critical reflection on the limitations of CER/WER metrics of evalution, see [Question of Semantics in CER/WER Evaluation](https://abdulfathah-jpg.github.io/portfolio/project1/2025/10/21/fourth-blog.html)

Reflecting on Mini-Project 1, it became clear how essential collaboration is in the field of digital humanities. Working as a group allowed us to share responsibilities and learn from each other’s abilities and insights. At the same time, the project highlighted the balance between using pre-existing tools and developing one’s own skills: for example, some Python scripts, like the ones used to compare mean Character Error Rate for OCR models or renaming scanned image set, were not originally ours, so we relied on ChatGPT and our foundational Python exercises to understand and apply them effectively. The project also exposed us to the limitations of automation: the fine-tuned segmentation model did not work perfectly on our manuscript, and while we felt the urge to manually fix every issue, we consciously restrained ourselves to observe how OCR workflows function when fully automated. Further, the digitization workflow required intelligent intervention form the researcher at almost every stage, such as during image quality control and deciding on the effectiveness of Python script. This experience showed us both the potential of machine learning to accelerate textual digitization as well its current constraints that may require oversight of a researcher. Overall, the project familiarized us with the entire digitization workflow and provided a tangible appreciation of how technical skills, collaborative effort, and critical reflection could converge in the preservation of cultural heritage.
